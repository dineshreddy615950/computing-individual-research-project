{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install roboflow\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"jEpWSuaWS6lKeyk8uJXY\")\n",
    "project = rf.workspace(\"plastic-b0ep9\").project(\"plastic-detection-2kkwi\")\n",
    "version = project.version(5)\n",
    "dataset = version.download(\"paligemma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import re\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import cv2\n",
    "import random\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "import warnings\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BoundingBox:\n",
    "    \"\"\"Data class for bounding box with validation\"\"\"\n",
    "    x1: float\n",
    "    y1: float\n",
    "    x2: float\n",
    "    y2: float\n",
    "    class_name: str\n",
    "    confidence: Optional[float] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate bounding box coordinates\"\"\"\n",
    "        if self.x1 >= self.x2 or self.y1 >= self.y2:\n",
    "            raise ValueError(f\"Invalid bbox coordinates: ({self.x1}, {self.y1}, {self.x2}, {self.y2})\")\n",
    "\n",
    "    @property\n",
    "    def width(self) -> float:\n",
    "        return self.x2 - self.x1\n",
    "\n",
    "    @property\n",
    "    def height(self) -> float:\n",
    "        return self.y2 - self.y1\n",
    "\n",
    "    @property\n",
    "    def area(self) -> float:\n",
    "        return self.width * self.height\n",
    "\n",
    "    @property\n",
    "    def aspect_ratio(self) -> float:\n",
    "        return self.width / self.height if self.height > 0 else 0\n",
    "\n",
    "    @property\n",
    "    def center(self) -> Tuple[float, float]:\n",
    "        return ((self.x1 + self.x2) / 2, (self.y1 + self.y2) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedPaliGemmaAnalyzer:\n",
    "    \"\"\"Enhanced analyzer for PaliGemma datasets with comprehensive analysis and visualization\"\"\"\n",
    "\n",
    "    def __init__(self, train_path: str, test_path: str, dataset_root: str,\n",
    "                 cache_dir: str = \".cache\", log_level: str = \"INFO\"):\n",
    "        self.train_path = Path(train_path)\n",
    "        self.test_path = Path(test_path)\n",
    "        self.dataset_root = Path(dataset_root)\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # Setup logging\n",
    "        logging.basicConfig(level=getattr(logging, log_level))\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Data storage\n",
    "        self.train_data = []\n",
    "        self.test_data = []\n",
    "        self.image_cache = {}\n",
    "\n",
    "        # Analysis results\n",
    "        self.stats = {}\n",
    "        self.class_info = {}\n",
    "\n",
    "        # Visualization settings\n",
    "        self.color_palette = [\n",
    "            '#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7',\n",
    "            '#DDA0DD', '#98D8C8', '#F7DC6F', '#BB8FCE', '#85C1E9',\n",
    "            '#F8C471', '#82E0AA', '#F1948A', '#85929E', '#D2B4DE'\n",
    "        ]\n",
    "        self.class_colors = {}\n",
    "\n",
    "    def _setup_class_colors(self):\n",
    "        \"\"\"Assign unique colors to each class\"\"\"\n",
    "        all_classes = set()\n",
    "        for item in self.train_data + self.test_data:\n",
    "            bboxes = self._parse_bbox_from_suffix(item['suffix'])\n",
    "            for bbox in bboxes:\n",
    "                all_classes.add(bbox.class_name)\n",
    "\n",
    "        for idx, class_name in enumerate(sorted(all_classes)):\n",
    "            self.class_colors[class_name] = self.color_palette[idx % len(self.color_palette)]\n",
    "\n",
    "        self.logger.info(f\"Assigned colors to {len(all_classes)} classes\")\n",
    "\n",
    "    def load_data(self) -> None:\n",
    "        \"\"\"Load and validate annotation files\"\"\"\n",
    "        self.logger.info(\"Loading annotation files...\")\n",
    "\n",
    "        # Load training data\n",
    "        if not self.train_path.exists():\n",
    "            raise FileNotFoundError(f\"Training file not found: {self.train_path}\")\n",
    "\n",
    "        with open(self.train_path, 'r', encoding='utf-8') as f:\n",
    "            self.train_data = [json.loads(line) for line in f]\n",
    "\n",
    "        # Load test data\n",
    "        if not self.test_path.exists():\n",
    "            raise FileNotFoundError(f\"Test file not found: {self.test_path}\")\n",
    "\n",
    "        with open(self.test_path, 'r', encoding='utf-8') as f:\n",
    "            self.test_data = [json.loads(line) for line in f]\n",
    "\n",
    "        self.logger.info(f\"Loaded {len(self.train_data)} train and {len(self.test_data)} test annotations\")\n",
    "\n",
    "        # Setup colors after loading data\n",
    "        self._setup_class_colors()\n",
    "\n",
    "        # Validate data\n",
    "        self._validate_data()\n",
    "\n",
    "    def _validate_data(self) -> None:\n",
    "        \"\"\"Validate loaded data for common issues\"\"\"\n",
    "        self.logger.info(\"Validating dataset...\")\n",
    "\n",
    "        issues = []\n",
    "\n",
    "        # Check for missing images\n",
    "        missing_images = 0\n",
    "        for split_name, data in [(\"train\", self.train_data), (\"test\", self.test_data)]:\n",
    "            for item in data:\n",
    "                img_path = self.dataset_root / item['image']\n",
    "                if not img_path.exists():\n",
    "                    missing_images += 1\n",
    "\n",
    "        if missing_images > 0:\n",
    "            issues.append(f\"Missing {missing_images} image files\")\n",
    "\n",
    "        # Check for duplicate images across splits\n",
    "        train_images = {item['image'] for item in self.train_data}\n",
    "        test_images = {item['image'] for item in self.test_data}\n",
    "        overlap = train_images.intersection(test_images)\n",
    "\n",
    "        if overlap:\n",
    "            issues.append(f\"Data leakage: {len(overlap)} images in both train and test\")\n",
    "\n",
    "        # Report issues\n",
    "        if issues:\n",
    "            for issue in issues:\n",
    "                self.logger.warning(issue)\n",
    "        else:\n",
    "            self.logger.info(\"✓ Data validation passed\")\n",
    "\n",
    "    def _parse_bbox_from_suffix(self, suffix: str) -> List[BoundingBox]:\n",
    "        \"\"\"Parse bounding boxes from PaliGemma suffix format with error handling\"\"\"\n",
    "        pattern = r'<loc(\\d{4})><loc(\\d{4})><loc(\\d{4})><loc(\\d{4})>\\s*([^<\\s]+)'\n",
    "        matches = re.findall(pattern, suffix)\n",
    "\n",
    "        bboxes = []\n",
    "        for match in matches:\n",
    "            try:\n",
    "                y1, x1, y2, x2, class_name = match\n",
    "                # Convert from normalized 1024 coordinates to relative coordinates\n",
    "                bbox = BoundingBox(\n",
    "                    x1=int(x1) / 1024,\n",
    "                    y1=int(y1) / 1024,\n",
    "                    x2=int(x2) / 1024,\n",
    "                    y2=int(y2) / 1024,\n",
    "                    class_name=class_name.strip()\n",
    "                )\n",
    "                bboxes.append(bbox)\n",
    "            except (ValueError, ZeroDivisionError) as e:\n",
    "                self.logger.warning(f\"Invalid bbox in suffix: {match}, error: {e}\")\n",
    "\n",
    "        return bboxes\n",
    "\n",
    "    def analyze_dataset_overview(self) -> Dict:\n",
    "        \"\"\"Comprehensive dataset overview analysis\"\"\"\n",
    "        self.logger.info(\"Analyzing dataset overview...\")\n",
    "\n",
    "        # Basic statistics\n",
    "        train_images = {item['image'] for item in self.train_data}\n",
    "        test_images = {item['image'] for item in self.test_data}\n",
    "        total_images = train_images.union(test_images)\n",
    "        overlap = train_images.intersection(test_images)\n",
    "\n",
    "        # Count annotations\n",
    "        train_annotations = sum(len(self._parse_bbox_from_suffix(item['suffix']))\n",
    "                              for item in self.train_data)\n",
    "        test_annotations = sum(len(self._parse_bbox_from_suffix(item['suffix']))\n",
    "                             for item in self.test_data)\n",
    "\n",
    "        # Class analysis\n",
    "        all_classes = set()\n",
    "        for item in self.train_data + self.test_data:\n",
    "            bboxes = self._parse_bbox_from_suffix(item['suffix'])\n",
    "            for bbox in bboxes:\n",
    "                all_classes.add(bbox.class_name)\n",
    "\n",
    "        overview = {\n",
    "            'total_images': len(total_images),\n",
    "            'train_images': len(train_images),\n",
    "            'test_images': len(test_images),\n",
    "            'image_overlap': len(overlap),\n",
    "            'total_annotations': train_annotations + test_annotations,\n",
    "            'train_annotations': train_annotations,\n",
    "            'test_annotations': test_annotations,\n",
    "            'num_classes': len(all_classes),\n",
    "            'classes': sorted(list(all_classes)),\n",
    "            'train_test_ratio': len(train_images) / len(test_images) if test_images else float('inf')\n",
    "        }\n",
    "\n",
    "        self.stats['overview'] = overview\n",
    "        return overview\n",
    "\n",
    "    def analyze_class_distribution(self) -> Dict:\n",
    "        \"\"\"Detailed class distribution analysis\"\"\"\n",
    "        self.logger.info(\"Analyzing class distribution...\")\n",
    "\n",
    "        train_classes = Counter()\n",
    "        test_classes = Counter()\n",
    "        class_stats = defaultdict(lambda: {\n",
    "            'train_count': 0, 'test_count': 0, 'total_count': 0,\n",
    "            'train_images': set(), 'test_images': set(),\n",
    "            'areas': [], 'aspect_ratios': []\n",
    "        })\n",
    "\n",
    "        # Analyze training data\n",
    "        for item in self.train_data:\n",
    "            bboxes = self._parse_bbox_from_suffix(item['suffix'])\n",
    "            for bbox in bboxes:\n",
    "                train_classes[bbox.class_name] += 1\n",
    "                class_stats[bbox.class_name]['train_count'] += 1\n",
    "                class_stats[bbox.class_name]['train_images'].add(item['image'])\n",
    "                class_stats[bbox.class_name]['areas'].append(bbox.area)\n",
    "                class_stats[bbox.class_name]['aspect_ratios'].append(bbox.aspect_ratio)\n",
    "\n",
    "        # Analyze test data\n",
    "        for item in self.test_data:\n",
    "            bboxes = self._parse_bbox_from_suffix(item['suffix'])\n",
    "            for bbox in bboxes:\n",
    "                test_classes[bbox.class_name] += 1\n",
    "                class_stats[bbox.class_name]['test_count'] += 1\n",
    "                class_stats[bbox.class_name]['test_images'].add(item['image'])\n",
    "                class_stats[bbox.class_name]['areas'].append(bbox.area)\n",
    "                class_stats[bbox.class_name]['aspect_ratios'].append(bbox.aspect_ratio)\n",
    "\n",
    "        # Calculate final statistics\n",
    "        for class_name in class_stats:\n",
    "            stats = class_stats[class_name]\n",
    "            stats['total_count'] = stats['train_count'] + stats['test_count']\n",
    "            stats['train_image_count'] = len(stats['train_images'])\n",
    "            stats['test_image_count'] = len(stats['test_images'])\n",
    "            stats['total_image_count'] = len(stats['train_images'].union(stats['test_images']))\n",
    "\n",
    "            if stats['areas']:\n",
    "                stats['avg_area'] = np.mean(stats['areas'])\n",
    "                stats['std_area'] = np.std(stats['areas'])\n",
    "                stats['avg_aspect_ratio'] = np.mean(stats['aspect_ratios'])\n",
    "\n",
    "            # Remove sets to make serializable\n",
    "            del stats['train_images'], stats['test_images']\n",
    "\n",
    "        distribution = {\n",
    "            'train_classes': dict(train_classes),\n",
    "            'test_classes': dict(test_classes),\n",
    "            'class_details': dict(class_stats)\n",
    "        }\n",
    "\n",
    "        self.stats['class_distribution'] = distribution\n",
    "        self.class_info = dict(class_stats)\n",
    "        return distribution\n",
    "\n",
    "    def analyze_spatial_distribution(self) -> Dict:\n",
    "        \"\"\"Analyze spatial distribution of objects\"\"\"\n",
    "        self.logger.info(\"Analyzing spatial distribution...\")\n",
    "\n",
    "        # Grid analysis (divide image into 3x3 grid)\n",
    "        grid_counts = np.zeros((3, 3))\n",
    "        center_positions = []\n",
    "        size_categories = {'small': 0, 'medium': 0, 'large': 0}\n",
    "\n",
    "        for item in self.train_data:\n",
    "            bboxes = self._parse_bbox_from_suffix(item['suffix'])\n",
    "            for bbox in bboxes:\n",
    "                # Grid position\n",
    "                grid_x = min(int(bbox.center[0] * 3), 2)\n",
    "                grid_y = min(int(bbox.center[1] * 3), 2)\n",
    "                grid_counts[grid_y, grid_x] += 1\n",
    "\n",
    "                # Center positions\n",
    "                center_positions.append(bbox.center)\n",
    "\n",
    "                # Size categorization\n",
    "                area = bbox.area\n",
    "                if area < 0.01:  # Less than 1% of image\n",
    "                    size_categories['small'] += 1\n",
    "                elif area < 0.1:  # Less than 10% of image\n",
    "                    size_categories['medium'] += 1\n",
    "                else:\n",
    "                    size_categories['large'] += 1\n",
    "\n",
    "        spatial = {\n",
    "            'grid_distribution': grid_counts.tolist(),\n",
    "            'center_positions': center_positions,\n",
    "            'size_categories': size_categories\n",
    "        }\n",
    "\n",
    "        self.stats['spatial'] = spatial\n",
    "        return spatial\n",
    "\n",
    "    def analyze_image_properties(self, sample_size: int = 200) -> Dict:\n",
    "        \"\"\"Analyze image properties with sampling for efficiency\"\"\"\n",
    "        self.logger.info(f\"Analyzing image properties (sample size: {sample_size})...\")\n",
    "\n",
    "        # Sample images for analysis\n",
    "        all_images = list({item['image'] for item in self.train_data + self.test_data})\n",
    "        sample_images = random.sample(all_images, min(sample_size, len(all_images)))\n",
    "\n",
    "        dimensions = []\n",
    "        formats = Counter()\n",
    "        file_sizes = []\n",
    "\n",
    "        for img_name in tqdm(sample_images, desc=\"Analyzing images\"):\n",
    "            img_path = self.dataset_root / img_name\n",
    "            if img_path.exists():\n",
    "                try:\n",
    "                    with Image.open(img_path) as img:\n",
    "                        dimensions.append((img.width, img.height))\n",
    "                        formats[img.format] += 1\n",
    "                        file_sizes.append(img_path.stat().st_size / 1024)  # KB\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Error reading {img_name}: {e}\")\n",
    "\n",
    "        if dimensions:\n",
    "            widths, heights = zip(*dimensions)\n",
    "            aspect_ratios = [w/h for w, h in dimensions]\n",
    "\n",
    "            properties = {\n",
    "                'sample_size': len(dimensions),\n",
    "                'width_stats': {\n",
    "                    'min': min(widths), 'max': max(widths),\n",
    "                    'mean': np.mean(widths), 'std': np.std(widths)\n",
    "                },\n",
    "                'height_stats': {\n",
    "                    'min': min(heights), 'max': max(heights),\n",
    "                    'mean': np.mean(heights), 'std': np.std(heights)\n",
    "                },\n",
    "                'aspect_ratio_stats': {\n",
    "                    'min': min(aspect_ratios), 'max': max(aspect_ratios),\n",
    "                    'mean': np.mean(aspect_ratios), 'std': np.std(aspect_ratios)\n",
    "                },\n",
    "                'formats': dict(formats),\n",
    "                'file_size_stats': {\n",
    "                    'min_kb': min(file_sizes) if file_sizes else 0,\n",
    "                    'max_kb': max(file_sizes) if file_sizes else 0,\n",
    "                    'mean_kb': np.mean(file_sizes) if file_sizes else 0\n",
    "                }\n",
    "            }\n",
    "        else:\n",
    "            properties = {'error': 'No valid images found'}\n",
    "\n",
    "        self.stats['image_properties'] = properties\n",
    "        return properties\n",
    "\n",
    "    def create_comprehensive_visualizations(self) -> None:\n",
    "        \"\"\"Create comprehensive visualization dashboard\"\"\"\n",
    "        self.logger.info(\"Creating comprehensive visualizations...\")\n",
    "\n",
    "        # Create large figure with subplots\n",
    "        fig = plt.figure(figsize=(20, 16))\n",
    "        gs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3)\n",
    "\n",
    "        # 1. Dataset Overview (2x2)\n",
    "        ax1 = fig.add_subplot(gs[0, :2])\n",
    "        self._plot_dataset_overview(ax1)\n",
    "\n",
    "        # 2. Class Distribution (2x2)\n",
    "        ax2 = fig.add_subplot(gs[0, 2:])\n",
    "        self._plot_class_distribution(ax2)\n",
    "\n",
    "        # 3. Spatial Distribution Heatmap\n",
    "        ax3 = fig.add_subplot(gs[1, 0])\n",
    "        self._plot_spatial_heatmap(ax3)\n",
    "\n",
    "        # 4. Object Size Distribution\n",
    "        ax4 = fig.add_subplot(gs[1, 1])\n",
    "        self._plot_size_distribution(ax4)\n",
    "\n",
    "        # 5. Aspect Ratio Distribution\n",
    "        ax5 = fig.add_subplot(gs[1, 2])\n",
    "        self._plot_aspect_ratio_distribution(ax5)\n",
    "\n",
    "        # 6. Objects per Image\n",
    "        ax6 = fig.add_subplot(gs[1, 3])\n",
    "        self._plot_objects_per_image(ax6)\n",
    "\n",
    "        # 7. Class Balance Analysis\n",
    "        ax7 = fig.add_subplot(gs[2, :2])\n",
    "        self._plot_class_balance(ax7)\n",
    "\n",
    "        # 8. Train vs Test Comparison\n",
    "        ax8 = fig.add_subplot(gs[2, 2:])\n",
    "        self._plot_train_test_comparison(ax8)\n",
    "\n",
    "        # 9. Center Position Scatter\n",
    "        ax9 = fig.add_subplot(gs[3, :2])\n",
    "        self._plot_center_positions(ax9)\n",
    "\n",
    "        # 10. Image Properties\n",
    "        ax10 = fig.add_subplot(gs[3, 2:])\n",
    "        self._plot_image_properties(ax10)\n",
    "\n",
    "        plt.suptitle('PaliGemma Dataset Analysis Dashboard', fontsize=20, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_dataset_overview(self, ax):\n",
    "        \"\"\"Plot dataset overview statistics\"\"\"\n",
    "        overview = self.stats['overview']\n",
    "\n",
    "        categories = ['Images', 'Annotations', 'Classes']\n",
    "        train_values = [overview['train_images'], overview['train_annotations'], overview['num_classes']]\n",
    "        test_values = [overview['test_images'], overview['test_annotations'], 0]  # Classes don't split\n",
    "\n",
    "        x = np.arange(len(categories))\n",
    "        width = 0.35\n",
    "\n",
    "        bars1 = ax.bar(x - width/2, train_values, width, label='Train', color='#3498db', alpha=0.8)\n",
    "        bars2 = ax.bar(x + width/2, test_values, width, label='Test', color='#e74c3c', alpha=0.8)\n",
    "\n",
    "        ax.set_title('Dataset Overview', fontweight='bold')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(categories)\n",
    "        ax.legend()\n",
    "\n",
    "        # Add value labels\n",
    "        for bars in [bars1, bars2]:\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                if height > 0:\n",
    "                    ax.annotate(f'{int(height)}',\n",
    "                               xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                               xytext=(0, 3), textcoords=\"offset points\",\n",
    "                               ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    def _plot_class_distribution(self, ax):\n",
    "        \"\"\"Plot class distribution\"\"\"\n",
    "        dist = self.stats['class_distribution']\n",
    "\n",
    "        classes = list(dist['train_classes'].keys())\n",
    "        train_counts = [dist['train_classes'][c] for c in classes]\n",
    "\n",
    "        colors = [self.class_colors[c] for c in classes]\n",
    "        bars = ax.bar(classes, train_counts, color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "        ax.set_title('Class Distribution (Training Set)', fontweight='bold')\n",
    "        ax.set_xlabel('Class')\n",
    "        ax.set_ylabel('Count')\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "        # Add percentage labels\n",
    "        total = sum(train_counts)\n",
    "        for bar, count in zip(bars, train_counts):\n",
    "            percentage = count / total * 100\n",
    "            ax.annotate(f'{percentage:.1f}%',\n",
    "                       xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
    "                       xytext=(0, 3), textcoords=\"offset points\",\n",
    "                       ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "    def _plot_spatial_heatmap(self, ax):\n",
    "        \"\"\"Plot spatial distribution heatmap\"\"\"\n",
    "        grid = np.array(self.stats['spatial']['grid_distribution'])\n",
    "\n",
    "        im = ax.imshow(grid, cmap='YlOrRd', interpolation='nearest')\n",
    "        ax.set_title('Object Position Heatmap', fontweight='bold')\n",
    "        ax.set_xlabel('X Position')\n",
    "        ax.set_ylabel('Y Position')\n",
    "\n",
    "        # Add text annotations\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                text = ax.text(j, i, f'{int(grid[i, j])}',\n",
    "                              ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "\n",
    "        plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "    def _plot_size_distribution(self, ax):\n",
    "        \"\"\"Plot object size distribution\"\"\"\n",
    "        areas = []\n",
    "        for item in self.train_data:\n",
    "            bboxes = self._parse_bbox_from_suffix(item['suffix'])\n",
    "            areas.extend([bbox.area * 100 for bbox in bboxes])  # Convert to percentage\n",
    "\n",
    "        ax.hist(areas, bins=30, alpha=0.7, color='#2ecc71', edgecolor='black')\n",
    "        ax.set_title('Object Size Distribution', fontweight='bold')\n",
    "        ax.set_xlabel('Size (% of image)')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.axvline(np.mean(areas), color='red', linestyle='--', linewidth=2,\n",
    "                  label=f'Mean: {np.mean(areas):.2f}%')\n",
    "        ax.legend()\n",
    "\n",
    "    def _plot_aspect_ratio_distribution(self, ax):\n",
    "        \"\"\"Plot aspect ratio distribution\"\"\"\n",
    "        ratios = []\n",
    "        for item in self.train_data:\n",
    "            bboxes = self._parse_bbox_from_suffix(item['suffix'])\n",
    "            ratios.extend([bbox.aspect_ratio for bbox in bboxes])\n",
    "\n",
    "        ax.hist(ratios, bins=30, alpha=0.7, color='#9b59b6', edgecolor='black')\n",
    "        ax.set_title('Aspect Ratio Distribution', fontweight='bold')\n",
    "        ax.set_xlabel('Width/Height Ratio')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.axvline(np.mean(ratios), color='red', linestyle='--', linewidth=2,\n",
    "                  label=f'Mean: {np.mean(ratios):.2f}')\n",
    "        ax.legend()\n",
    "\n",
    "    def _plot_objects_per_image(self, ax):\n",
    "        \"\"\"Plot objects per image distribution\"\"\"\n",
    "        objects_per_image = []\n",
    "        for item in self.train_data:\n",
    "            bboxes = self._parse_bbox_from_suffix(item['suffix'])\n",
    "            objects_per_image.append(len(bboxes))\n",
    "\n",
    "        ax.hist(objects_per_image, bins=20, alpha=0.7, color='#f39c12', edgecolor='black')\n",
    "        ax.set_title('Objects per Image', fontweight='bold')\n",
    "        ax.set_xlabel('Number of Objects')\n",
    "        ax.set_ylabel('Number of Images')\n",
    "        ax.axvline(np.mean(objects_per_image), color='red', linestyle='--', linewidth=2,\n",
    "                  label=f'Mean: {np.mean(objects_per_image):.1f}')\n",
    "        ax.legend()\n",
    "\n",
    "    def _plot_class_balance(self, ax):\n",
    "        \"\"\"Plot class balance analysis\"\"\"\n",
    "        dist = self.stats['class_distribution']\n",
    "        classes = list(dist['train_classes'].keys())\n",
    "        counts = [dist['train_classes'][c] for c in classes]\n",
    "\n",
    "        # Calculate balance metrics\n",
    "        total = sum(counts)\n",
    "        percentages = [c/total * 100 for c in counts]\n",
    "\n",
    "        # Create horizontal bar chart\n",
    "        y_pos = np.arange(len(classes))\n",
    "        colors = [self.class_colors[c] for c in classes]\n",
    "\n",
    "        bars = ax.barh(y_pos, percentages, color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "        ax.set_title('Class Balance Analysis', fontweight='bold')\n",
    "        ax.set_xlabel('Percentage of Total Annotations')\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(classes)\n",
    "\n",
    "        # Add percentage labels\n",
    "        for i, (bar, pct) in enumerate(zip(bars, percentages)):\n",
    "            ax.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2,\n",
    "                   f'{pct:.1f}%', ha='left', va='center', fontweight='bold')\n",
    "\n",
    "    def _plot_train_test_comparison(self, ax):\n",
    "        \"\"\"Plot train vs test comparison\"\"\"\n",
    "        dist = self.stats['class_distribution']\n",
    "        classes = list(dist['train_classes'].keys())\n",
    "\n",
    "        train_counts = [dist['train_classes'][c] for c in classes]\n",
    "        test_counts = [dist['test_classes'].get(c, 0) for c in classes]\n",
    "\n",
    "        x = np.arange(len(classes))\n",
    "        width = 0.35\n",
    "\n",
    "        ax.bar(x - width/2, train_counts, width, label='Train', color='#3498db', alpha=0.8)\n",
    "        ax.bar(x + width/2, test_counts, width, label='Test', color='#e74c3c', alpha=0.8)\n",
    "\n",
    "        ax.set_title('Train vs Test Distribution by Class', fontweight='bold')\n",
    "        ax.set_xlabel('Class')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(classes, rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "\n",
    "    def _plot_center_positions(self, ax):\n",
    "        \"\"\"Plot object center positions\"\"\"\n",
    "        centers = self.stats['spatial']['center_positions']\n",
    "        x_coords, y_coords = zip(*centers) if centers else ([], [])\n",
    "\n",
    "        # Create scatter plot with density\n",
    "        ax.scatter(x_coords, y_coords, alpha=0.5, s=10, c='#34495e')\n",
    "        ax.set_title('Object Center Positions', fontweight='bold')\n",
    "        ax.set_xlabel('X Position (normalized)')\n",
    "        ax.set_ylabel('Y Position (normalized)')\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        # Invert y-axis to match image coordinates\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "    def _plot_image_properties(self, ax):\n",
    "        \"\"\"Plot image properties\"\"\"\n",
    "        props = self.stats.get('image_properties', {})\n",
    "\n",
    "        if 'width_stats' in props:\n",
    "            # Plot width vs height scatter\n",
    "            # This would need actual image data, so we'll show a summary instead\n",
    "\n",
    "            stats_text = f\"\"\"Image Properties Summary:\n",
    "\n",
    "Width: {props['width_stats']['mean']:.0f} ± {props['width_stats']['std']:.0f}\n",
    "Height: {props['height_stats']['mean']:.0f} ± {props['height_stats']['std']:.0f}\n",
    "Aspect Ratio: {props['aspect_ratio_stats']['mean']:.2f} ± {props['aspect_ratio_stats']['std']:.2f}\n",
    "\n",
    "File Size: {props['file_size_stats']['mean_kb']:.1f} KB (avg)\n",
    "\n",
    "Formats: {', '.join(props['formats'].keys())}\n",
    "Sample Size: {props['sample_size']} images\"\"\"\n",
    "\n",
    "            ax.text(0.05, 0.95, stats_text, transform=ax.transAxes, fontsize=10,\n",
    "                   verticalalignment='top', fontfamily='monospace',\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='lightgray', alpha=0.8))\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'Image properties\\nnot analyzed',\n",
    "                   ha='center', va='center', transform=ax.transAxes)\n",
    "\n",
    "        ax.set_title('Image Properties', fontweight='bold')\n",
    "        ax.axis('off')\n",
    "\n",
    "    def visualize_sample_images(self, num_samples: int = 12, split: str = 'train',\n",
    "                               save_path: Optional[str] = None) -> None:\n",
    "        \"\"\"Enhanced sample image visualization\"\"\"\n",
    "        self.logger.info(f\"Visualizing {num_samples} sample images from {split} set...\")\n",
    "\n",
    "        data = self.train_data if split == 'train' else self.test_data\n",
    "        samples = random.sample(data, min(num_samples, len(data)))\n",
    "\n",
    "        # Create grid\n",
    "        cols = 4\n",
    "        rows = (num_samples + cols - 1) // cols\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(20, rows * 5))\n",
    "        axes = axes.flatten() if num_samples > 1 else [axes]\n",
    "\n",
    "        for idx, (ax, item) in enumerate(zip(axes[:num_samples], samples)):\n",
    "            img_path = self.dataset_root / item['image']\n",
    "\n",
    "            try:\n",
    "                # Load and display image\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img_array = np.array(img)\n",
    "                ax.imshow(img_array)\n",
    "\n",
    "                # Parse and draw bounding boxes\n",
    "                bboxes = self._parse_bbox_from_suffix(item['suffix'])\n",
    "                img_height, img_width = img_array.shape[:2]\n",
    "\n",
    "                class_counts = Counter([bbox.class_name for bbox in bboxes])\n",
    "\n",
    "                for bbox in bboxes:\n",
    "                    # Convert to absolute coordinates\n",
    "                    x1 = int(bbox.x1 * img_width)\n",
    "                    y1 = int(bbox.y1 * img_height)\n",
    "                    x2 = int(bbox.x2 * img_width)\n",
    "                    y2 = int(bbox.y2 * img_height)\n",
    "\n",
    "                    # Get color\n",
    "                    color = self.class_colors.get(bbox.class_name, '#FF0000')\n",
    "                    color_rgb = [int(color[1:3], 16)/255, int(color[3:5], 16)/255, int(color[5:7], 16)/255]\n",
    "\n",
    "                    # Draw rectangle\n",
    "                    rect = patches.Rectangle(\n",
    "                        (x1, y1), x2-x1, y2-y1,\n",
    "                        linewidth=2, edgecolor=color_rgb, facecolor='none'\n",
    "                    )\n",
    "                    ax.add_patch(rect)\n",
    "\n",
    "                    # Add label with background\n",
    "                    ax.text(x1, y1-5, bbox.class_name,\n",
    "                           bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=color, alpha=0.8),\n",
    "                           fontsize=9, color='white', weight='bold')\n",
    "\n",
    "                # Enhanced title with more info\n",
    "                class_summary = ', '.join([f\"{cls}({cnt})\" for cls, cnt in class_counts.most_common()])\n",
    "                title = f\"{Path(item['image']).name}\\n{len(bboxes)} objects: {class_summary}\"\n",
    "                ax.set_title(title, fontsize=10, pad=10)\n",
    "                ax.axis('off')\n",
    "\n",
    "            except Exception as e:\n",
    "                ax.text(0.5, 0.5, f\"Error loading image:\\n{str(e)}\",\n",
    "                       ha='center', va='center', transform=ax.transAxes,\n",
    "                       bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='red', alpha=0.3))\n",
    "                ax.set_title(f\"Error: {Path(item['image']).name}\", fontsize=10)\n",
    "                ax.axis('off')\n",
    "\n",
    "        # Hide unused subplots\n",
    "        for idx in range(num_samples, len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "\n",
    "        plt.suptitle(f'Sample Images from {split.title()} Set', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            self.logger.info(f\"Saved visualization to {save_path}\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def generate_detailed_report(self, output_file: Optional[str] = None) -> Dict:\n",
    "        \"\"\"Generate comprehensive analysis report\"\"\"\n",
    "        self.logger.info(\"Generating comprehensive analysis report...\")\n",
    "\n",
    "        # Perform all analyses\n",
    "        self.load_data()\n",
    "\n",
    "        # Run analyses\n",
    "        overview = self.analyze_dataset_overview()\n",
    "        class_dist = self.analyze_class_distribution()\n",
    "        spatial = self.analyze_spatial_distribution()\n",
    "        image_props = self.analyze_image_properties()\n",
    "\n",
    "        # Print summary\n",
    "        self._print_summary_report()\n",
    "\n",
    "        # Create visualizations\n",
    "        self.create_comprehensive_visualizations()\n",
    "\n",
    "        # Show sample images\n",
    "        self.visualize_sample_images(num_samples=8, split='train')\n",
    "\n",
    "        # Save report if requested\n",
    "        if output_file:\n",
    "            report_data = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'overview': overview,\n",
    "                'class_distribution': class_dist,\n",
    "                'spatial_distribution': spatial,\n",
    "                'image_properties': image_props\n",
    "            }\n",
    "\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(report_data, f, indent=2, default=str)\n",
    "            self.logger.info(f\"Report saved to {output_file}\")\n",
    "\n",
    "        return self.stats\n",
    "\n",
    "    def _print_summary_report(self):\n",
    "        \"\"\"Print a formatted summary report\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\" \" * 25 + \"PALIGEMMA DATASET ANALYSIS REPORT\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        overview = self.stats['overview']\n",
    "\n",
    "        print(f\"\\n📊 DATASET OVERVIEW\")\n",
    "        print(f\"   Total Images: {overview['total_images']:,}\")\n",
    "        print(f\"   Total Annotations: {overview['total_annotations']:,}\")\n",
    "        print(f\"   Number of Classes: {overview['num_classes']}\")\n",
    "        print(f\"   Train/Test Ratio: {overview['train_test_ratio']:.2f}\")\n",
    "\n",
    "        if overview['image_overlap'] > 0:\n",
    "            print(f\"   ⚠️  Data Leakage: {overview['image_overlap']} overlapping images\")\n",
    "        else:\n",
    "            print(f\"   ✅ No data leakage detected\")\n",
    "\n",
    "        print(f\"\\n🎯 CLASS STATISTICS\")\n",
    "        dist = self.stats['class_distribution']\n",
    "        total_annotations = overview['total_annotations']\n",
    "\n",
    "        for class_name, details in self.class_info.items():\n",
    "            percentage = (details['total_count'] / total_annotations) * 100\n",
    "            print(f\"   {class_name}: {details['total_count']} annotations ({percentage:.1f}%) \"\n",
    "                  f\"in {details['total_image_count']} images\")\n",
    "\n",
    "        print(f\"\\n📐 SPATIAL DISTRIBUTION\")\n",
    "        spatial = self.stats['spatial']\n",
    "        size_cats = spatial['size_categories']\n",
    "        total_objects = sum(size_cats.values())\n",
    "\n",
    "        for size, count in size_cats.items():\n",
    "            percentage = (count / total_objects) * 100 if total_objects > 0 else 0\n",
    "            print(f\"   {size.title()} objects: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with enhanced features\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize analyzer\n",
    "    analyzer = EnhancedPaliGemmaAnalyzer(\n",
    "        train_path=\"Plastic-Detection-5/dataset/_annotations.train.jsonl\",\n",
    "        test_path=\"Plastic-Detection-5/dataset/_annotations.test.jsonl\",\n",
    "        dataset_root=\"Plastic-Detection-5/dataset\",\n",
    "        log_level=\"INFO\"\n",
    "    )\n",
    "\n",
    "    # Generate comprehensive report\n",
    "    stats = analyzer.generate_detailed_report(output_file=\"dataset_analysis_report.json\")\n",
    "\n",
    "    print(\"\\n✅ Analysis complete! Check the visualizations and saved report.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  transformers==4.47.0 datasets peft bitsandbytes accelerate torchmetrics supervision\n",
    "!pip install  pillow opencv-python-headless matplotlib seaborn\n",
    "!pip install  supervision peft bitsandbytes transformers==4.47.0 huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import supervision as sv\n",
    "from huggingface_hub import login\n",
    "from transformers import (\n",
    "    PaliGemmaProcessor,\n",
    "    PaliGemmaForConditionalGeneration,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== CELL 3: AUTHENTICATION WITH YOUR TOKEN ======================\n",
    "print(\"Setting up HuggingFace authentication...\")\n",
    "\n",
    "# Login to HuggingFace\n",
    "# Use Colab's Secrets feature for storing your Hugging Face token\n",
    "# Go to the \"🔑\" icon in the left sidebar, click \"Add new secret\",\n",
    "# set Name as HF_TOKEN and Value as your token.\n",
    "from google.colab import userdata\n",
    "\n",
    "HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\"Hugging Face token not found in Colab Secrets. Please add it.\")\n",
    "\n",
    "login(token=HF_TOKEN)\n",
    "print(\"✅ Logged in to HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencing\n",
    "\n",
    "## Paligemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "# Core ML libraries\n",
    "from transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration\n",
    "from peft import PeftModel\n",
    "import supervision as sv\n",
    "\n",
    "# Suppress deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('default')\n",
    "\n",
    "@dataclass\n",
    "class SingleModelConfig:\n",
    "    \"\"\"Configuration for single model inference\"\"\"\n",
    "    # Model path - CHANGE THIS TO YOUR MODEL\n",
    "    model_path: str = \"/content/drive/MyDrive/paligemma_1\"  # Change to paligemma_2 if needed\n",
    "    model_name: str = \"PaliGemma_1\"  # Friendly name\n",
    "\n",
    "    # Dataset paths\n",
    "    test_path: str = \"Plastic-Detection-5/dataset/_annotations.test.jsonl\"\n",
    "    dataset_root: str = \"Plastic-Detection-5/dataset\"\n",
    "\n",
    "    # Settings\n",
    "    num_images: int = 10\n",
    "\n",
    "    # Generation settings\n",
    "    max_new_tokens: int = 256\n",
    "    do_sample: bool = False\n",
    "    temperature: float = 0.0\n",
    "    num_beams: int = 1\n",
    "\n",
    "    # Device settings\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    torch_dtype: torch.dtype = torch.bfloat16\n",
    "\n",
    "    # Output settings\n",
    "    output_dir: str = \"single_model_results\"\n",
    "\n",
    "class SingleModelInference:\n",
    "    \"\"\"Simple single model inference\"\"\"\n",
    "\n",
    "    def __init__(self, config: SingleModelConfig):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "        self.test_data = []\n",
    "        self.classes = ['plastic']\n",
    "        self.results = []\n",
    "\n",
    "        # Create output directory\n",
    "        Path(self.config.output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "        print(\"=\"*60)\n",
    "        print(\"🎯 SINGLE MODEL PALIGEMMA INFERENCE\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"📁 Model: {self.config.model_name}\")\n",
    "        print(f\"📂 Path: {self.config.model_path}\")\n",
    "        print(f\"🖼️  Images: {self.config.num_images}\")\n",
    "        print(f\"💾 Device: {self.config.device}\")\n",
    "        if torch.cuda.is_available():\n",
    "            total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            print(f\"🔥 GPU Memory: {total_memory:.1f}GB available\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "    def cleanup_memory(self):\n",
    "        \"\"\"Clean up GPU memory\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "        gc.collect()\n",
    "\n",
    "    def detect_base_model(self) -> str:\n",
    "        \"\"\"Auto-detect base model\"\"\"\n",
    "        try:\n",
    "            config_paths = [\n",
    "                Path(self.config.model_path) / \"training_config.json\",\n",
    "                Path(self.config.model_path) / \"adapter_config.json\",\n",
    "                Path(self.config.model_path) / \"config.json\"\n",
    "            ]\n",
    "\n",
    "            for config_path in config_paths:\n",
    "                if config_path.exists():\n",
    "                    print(f\"   📄 Found config: {config_path.name}\")\n",
    "                    with open(config_path, 'r') as f:\n",
    "                        config = json.load(f)\n",
    "\n",
    "                    # Look for base model\n",
    "                    keys = ['model_id', 'base_model', '_name_or_path', 'model_name_or_path']\n",
    "                    for key in keys:\n",
    "                        if key in config and config[key]:\n",
    "                            print(f\"   🎯 Found base model: {config[key]}\")\n",
    "                            return config[key]\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️  Error reading config: {e}\")\n",
    "\n",
    "        # Fallback based on path\n",
    "        if \"paligemma2\" in self.config.model_path.lower() or \"paligemma_2\" in self.config.model_path:\n",
    "            fallback = \"google/paligemma2-3b-pt-224\"\n",
    "        else:\n",
    "            fallback = \"google/paligemma-3b-pt-224\"\n",
    "\n",
    "        print(f\"   🔄 Using fallback: {fallback}\")\n",
    "        return fallback\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Load the model\"\"\"\n",
    "        print(\"\\n🔄 Loading model...\")\n",
    "\n",
    "        # Check if path exists\n",
    "        if not Path(self.config.model_path).exists():\n",
    "            raise FileNotFoundError(f\"❌ Model path not found: {self.config.model_path}\")\n",
    "\n",
    "        print(f\"   ✅ Model path exists: {self.config.model_path}\")\n",
    "\n",
    "        try:\n",
    "            # Detect base model\n",
    "            base_model_id = self.detect_base_model()\n",
    "\n",
    "            # Load processor\n",
    "            print(\"   📝 Loading processor...\")\n",
    "            try:\n",
    "                self.processor = PaliGemmaProcessor.from_pretrained(self.config.model_path)\n",
    "                print(\"   ✅ Loaded processor from fine-tuned model\")\n",
    "            except:\n",
    "                self.processor = PaliGemmaProcessor.from_pretrained(base_model_id)\n",
    "                print(\"   ✅ Loaded processor from base model\")\n",
    "\n",
    "            # Load base model\n",
    "            print(f\"   🧠 Loading base model: {base_model_id}\")\n",
    "            base_model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "                base_model_id,\n",
    "                torch_dtype=self.config.torch_dtype,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "\n",
    "            # Load LoRA weights\n",
    "            print(\"   🔧 Loading LoRA weights...\")\n",
    "            self.model = PeftModel.from_pretrained(base_model, self.config.model_path)\n",
    "\n",
    "            # Try to merge for faster inference\n",
    "            try:\n",
    "                print(\"   🔗 Merging LoRA weights...\")\n",
    "                self.model = self.model.merge_and_unload()\n",
    "                print(\"   ✅ LoRA weights merged!\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️  Could not merge: {e}\")\n",
    "\n",
    "            self.model.eval()\n",
    "\n",
    "            # Show memory usage\n",
    "            if torch.cuda.is_available():\n",
    "                allocated = torch.cuda.memory_allocated() / 1e9\n",
    "                print(f\"   💾 GPU Memory: {allocated:.2f}GB\")\n",
    "\n",
    "            print(\"   ✅ Model loaded successfully!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error loading model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def load_test_data(self):\n",
    "        \"\"\"Load test dataset\"\"\"\n",
    "        print(\"\\n📂 Loading test dataset...\")\n",
    "\n",
    "        if not Path(self.config.test_path).exists():\n",
    "            raise FileNotFoundError(f\"❌ Test file not found: {self.config.test_path}\")\n",
    "\n",
    "        # Load data\n",
    "        with open(self.config.test_path, 'r', encoding='utf-8') as f:\n",
    "            all_data = [json.loads(line) for line in f]\n",
    "\n",
    "        # Select images\n",
    "        self.test_data = all_data[:self.config.num_images]\n",
    "        print(f\"   📊 Selected {len(self.test_data)} images\")\n",
    "\n",
    "        # Pre-load images\n",
    "        print(\"   🖼️  Loading images...\")\n",
    "        self.images = []\n",
    "        self.image_info = []\n",
    "\n",
    "        for item in tqdm(self.test_data, desc=\"Loading images\"):\n",
    "            img_path = Path(self.config.dataset_root) / item['image']\n",
    "            if img_path.exists():\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "                # Resize if needed\n",
    "                max_size = 1024\n",
    "                if max(image.size) > max_size:\n",
    "                    ratio = max_size / max(image.size)\n",
    "                    new_size = (int(image.width * ratio), int(image.height * ratio))\n",
    "                    image = image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "                self.images.append(image)\n",
    "                self.image_info.append({\n",
    "                    'path': item['image'],\n",
    "                    'prompt': \"<image>\" + item['prefix'],\n",
    "                    'gt_text': item['suffix'],\n",
    "                    'size': image.size\n",
    "                })\n",
    "\n",
    "        print(f\"   ✅ Loaded {len(self.images)} images\")\n",
    "\n",
    "    def generate_prediction(self, image: Image.Image, prompt: str) -> str:\n",
    "        \"\"\"Generate prediction for image\"\"\"\n",
    "        try:\n",
    "            # Prepare inputs\n",
    "            inputs = self.processor(\n",
    "                text=prompt,\n",
    "                images=image,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=256\n",
    "            ).to(self.config.device)\n",
    "\n",
    "            # Convert to correct dtype\n",
    "            if 'pixel_values' in inputs:\n",
    "                inputs['pixel_values'] = inputs['pixel_values'].to(self.config.torch_dtype)\n",
    "\n",
    "            prefix_length = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "            # Generate\n",
    "            with torch.inference_mode():\n",
    "                generation = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=self.config.max_new_tokens,\n",
    "                    do_sample=self.config.do_sample,\n",
    "                    num_beams=self.config.num_beams,\n",
    "                    temperature=self.config.temperature,\n",
    "                    pad_token_id=self.processor.tokenizer.eos_token_id,\n",
    "                    eos_token_id=self.processor.tokenizer.eos_token_id,\n",
    "                    use_cache=True,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "\n",
    "            # Decode\n",
    "            new_tokens = generation[0][prefix_length:]\n",
    "            generated_text = self.processor.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "            return generated_text.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Generation error: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    def parse_detections(self, result_text: str, image_size: Tuple[int, int]) -> sv.Detections:\n",
    "        \"\"\"Parse detections using updated supervision API\"\"\"\n",
    "        try:\n",
    "            if \"Error\" in result_text:\n",
    "                return sv.Detections.empty()\n",
    "\n",
    "            # Updated to use from_vlm instead of from_lmm\n",
    "            detections = sv.Detections.from_vlm(\n",
    "                vlm='paligemma',  # Changed from 'lmm' to 'vlm'\n",
    "                result=result_text,\n",
    "                resolution_wh=image_size,\n",
    "                classes=self.classes\n",
    "            )\n",
    "\n",
    "            if len(detections) > 0:\n",
    "                detections.confidence = np.ones(len(detections))\n",
    "                if not hasattr(detections, 'class_id') or detections.class_id is None:\n",
    "                    detections.class_id = np.zeros(len(detections), dtype=int)\n",
    "\n",
    "            return detections\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️  Parse error for '{result_text[:50]}...': {e}\")\n",
    "            return sv.Detections.empty()\n",
    "\n",
    "    def run_inference(self):\n",
    "        \"\"\"Run inference on all images\"\"\"\n",
    "        print(\"\\n🎯 Running inference...\")\n",
    "\n",
    "        self.results = []\n",
    "\n",
    "        for i, (image, info) in enumerate(tqdm(zip(self.images, self.image_info),\n",
    "                                              desc=\"Processing images\",\n",
    "                                              total=len(self.images))):\n",
    "            try:\n",
    "                # Generate prediction\n",
    "                pred_text = self.generate_prediction(image, info['prompt'])\n",
    "\n",
    "                # Parse detections\n",
    "                detections = self.parse_detections(pred_text, info['size'])\n",
    "                gt_detections = self.parse_detections(info['gt_text'], info['size'])\n",
    "\n",
    "                # Store result\n",
    "                result = {\n",
    "                    'image_path': info['path'],\n",
    "                    'image': image,\n",
    "                    'prompt': info['prompt'],\n",
    "                    'generated_text': pred_text,\n",
    "                    'ground_truth_text': info['gt_text'],\n",
    "                    'predicted_detections': detections,\n",
    "                    'ground_truth_detections': gt_detections,\n",
    "                    'predicted_count': len(detections),\n",
    "                    'ground_truth_count': len(gt_detections)\n",
    "                }\n",
    "\n",
    "                self.results.append(result)\n",
    "\n",
    "                # Print progress\n",
    "                if (i + 1) % 5 == 0 or i == 0:\n",
    "                    print(f\"   📊 Image {i+1}: Generated '{pred_text[:50]}...', Detections: {len(detections)}\")\n",
    "\n",
    "                # Clear cache\n",
    "                if (i + 1) % 3 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Error processing image {i}: {e}\")\n",
    "                # Add empty result\n",
    "                self.results.append({\n",
    "                    'image_path': info['path'],\n",
    "                    'image': image,\n",
    "                    'generated_text': f\"Error: {str(e)}\",\n",
    "                    'predicted_detections': sv.Detections.empty(),\n",
    "                    'ground_truth_detections': sv.Detections.empty(),\n",
    "                    'predicted_count': 0,\n",
    "                    'ground_truth_count': 0\n",
    "                })\n",
    "\n",
    "        total_predictions = sum(r['predicted_count'] for r in self.results)\n",
    "        total_ground_truth = sum(r['ground_truth_count'] for r in self.results)\n",
    "\n",
    "        print(f\"\\n   📊 Inference completed!\")\n",
    "        print(f\"   🎯 Total predictions: {total_predictions}\")\n",
    "        print(f\"   📋 Total ground truth: {total_ground_truth}\")\n",
    "        print(f\"   📈 Average per image: {total_predictions/len(self.results):.2f}\")\n",
    "\n",
    "    def create_visualization(self):\n",
    "        \"\"\"Create visualization of results\"\"\"\n",
    "        print(\"\\n🎨 Creating visualization...\")\n",
    "\n",
    "        # Create grid\n",
    "        cols = 2  # Ground Truth | Prediction\n",
    "        rows = len(self.results)\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(12, 4*rows))\n",
    "\n",
    "        if rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "\n",
    "        for i, result in enumerate(self.results):\n",
    "            # Ground Truth\n",
    "            ax_gt = axes[i, 0] if rows > 1 else axes[0]\n",
    "            ax_gt.imshow(result['image'])\n",
    "\n",
    "            # Draw GT boxes in green\n",
    "            for bbox in result['ground_truth_detections'].xyxy:\n",
    "                x1, y1, x2, y2 = bbox\n",
    "                rect = plt.Rectangle((x1, y1), x2-x1, y2-y1,\n",
    "                                   linewidth=2, edgecolor='green',\n",
    "                                   facecolor='none', alpha=0.8)\n",
    "                ax_gt.add_patch(rect)\n",
    "\n",
    "            ax_gt.set_title(f'Ground Truth\\nDetections: {result[\"ground_truth_count\"]}')\n",
    "            ax_gt.axis('off')\n",
    "\n",
    "            # Predictions\n",
    "            ax_pred = axes[i, 1] if rows > 1 else axes[1]\n",
    "            ax_pred.imshow(result['image'])\n",
    "\n",
    "            # Draw prediction boxes in red\n",
    "            for bbox in result['predicted_detections'].xyxy:\n",
    "                x1, y1, x2, y2 = bbox\n",
    "                rect = plt.Rectangle((x1, y1), x2-x1, y2-y1,\n",
    "                                   linewidth=2, edgecolor='red',\n",
    "                                   facecolor='none', alpha=0.8)\n",
    "                ax_pred.add_patch(rect)\n",
    "\n",
    "            ax_pred.set_title(f'Prediction\\nDetections: {result[\"predicted_count\"]}')\n",
    "            ax_pred.axis('off')\n",
    "\n",
    "        plt.suptitle(f'{self.config.model_name} Results', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save\n",
    "        vis_path = Path(self.config.output_dir) / f\"{self.config.model_name}_results.png\"\n",
    "        plt.savefig(vis_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"   💾 Saved: {vis_path}\")\n",
    "        plt.show()\n",
    "\n",
    "    def print_summary(self):\n",
    "        \"\"\"Print detailed summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"📊 INFERENCE SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        print(f\"\\n🎯 Model: {self.config.model_name}\")\n",
    "        print(f\"📂 Path: {self.config.model_path}\")\n",
    "        print(f\"🖼️  Images processed: {len(self.results)}\")\n",
    "\n",
    "        # Statistics\n",
    "        total_pred = sum(r['predicted_count'] for r in self.results)\n",
    "        total_gt = sum(r['ground_truth_count'] for r in self.results)\n",
    "\n",
    "        print(f\"\\n📈 Detection Statistics:\")\n",
    "        print(f\"   Predicted: {total_pred} ({total_pred/len(self.results):.2f} avg)\")\n",
    "        print(f\"   Ground Truth: {total_gt} ({total_gt/len(self.results):.2f} avg)\")\n",
    "\n",
    "        # Per-image breakdown\n",
    "        print(f\"\\n📋 Per-Image Results:\")\n",
    "        print(\"Image\".ljust(30) + \"GT\".ljust(6) + \"Pred\".ljust(6) + \"Generated Text\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        for result in self.results:\n",
    "            img_name = Path(result['image_path']).name[:25]\n",
    "            gt_count = result['ground_truth_count']\n",
    "            pred_count = result['predicted_count']\n",
    "            gen_text = result['generated_text'][:30]\n",
    "\n",
    "            print(f\"{img_name}...\".ljust(30) +\n",
    "                  f\"{gt_count}\".ljust(6) +\n",
    "                  f\"{pred_count}\".ljust(6) +\n",
    "                  f\"{gen_text}...\")\n",
    "\n",
    "        print(\"=\"*60)\n",
    "\n",
    "    def save_results(self):\n",
    "        \"\"\"Save detailed results\"\"\"\n",
    "        print(\"\\n💾 Saving results...\")\n",
    "\n",
    "        # Prepare data for JSON\n",
    "        json_results = []\n",
    "        for result in self.results:\n",
    "            json_result = {\n",
    "                'image_path': result['image_path'],\n",
    "                'prompt': result['prompt'],\n",
    "                'generated_text': result['generated_text'],\n",
    "                'ground_truth_text': result['ground_truth_text'],\n",
    "                'predicted_count': result['predicted_count'],\n",
    "                'ground_truth_count': result['ground_truth_count'],\n",
    "                'predicted_boxes': result['predicted_detections'].xyxy.tolist() if len(result['predicted_detections']) > 0 else [],\n",
    "                'ground_truth_boxes': result['ground_truth_detections'].xyxy.tolist() if len(result['ground_truth_detections']) > 0 else []\n",
    "            }\n",
    "            json_results.append(json_result)\n",
    "\n",
    "        # Save\n",
    "        results_path = Path(self.config.output_dir) / f\"{self.config.model_name}_detailed_results.json\"\n",
    "        with open(results_path, 'w') as f:\n",
    "            json.dump(json_results, f, indent=2)\n",
    "\n",
    "        print(f\"   💾 Saved: {results_path}\")\n",
    "\n",
    "    def run_complete_inference(self):\n",
    "        \"\"\"Run complete inference pipeline\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            # Load model and data\n",
    "            self.load_model()\n",
    "            self.load_test_data()\n",
    "\n",
    "            # Run inference\n",
    "            self.run_inference()\n",
    "\n",
    "            # Create outputs\n",
    "            self.create_visualization()\n",
    "            self.print_summary()\n",
    "            self.save_results()\n",
    "\n",
    "            total_time = time.time() - start_time\n",
    "            print(f\"\\n🎉 INFERENCE COMPLETED!\")\n",
    "            print(f\"⏱️  Total time: {total_time:.1f} seconds\")\n",
    "            print(f\"📁 Results saved in: {self.config.output_dir}/\")\n",
    "\n",
    "            return self.results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n💥 Inference failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "# Main functions\n",
    "def test_single_model(model_path: str,\n",
    "                     model_name: str = None,\n",
    "                     num_images: int = 10):\n",
    "    \"\"\"\n",
    "    Test a single model\n",
    "\n",
    "    Args:\n",
    "        model_path: Path to your model (e.g., \"/content/drive/MyDrive/paligemma_1\")\n",
    "        model_name: Friendly name (optional)\n",
    "        num_images: Number of images to test\n",
    "    \"\"\"\n",
    "\n",
    "    if model_name is None:\n",
    "        model_name = Path(model_path).name\n",
    "\n",
    "    config = SingleModelConfig(\n",
    "        model_path=model_path,\n",
    "        model_name=model_name,\n",
    "        num_images=num_images\n",
    "    )\n",
    "\n",
    "    inferencer = SingleModelInference(config)\n",
    "    return inferencer.run_complete_inference()\n",
    "\n",
    "# Quick test functions\n",
    "def test_paligemma_1(num_images: int = 10):\n",
    "    \"\"\"Test PaliGemma_1 model\"\"\"\n",
    "    return test_single_model(\n",
    "        model_path=\"/content/drive/MyDrive/paligemma_1\",\n",
    "        model_name=\"PaliGemma_1\",\n",
    "        num_images=num_images\n",
    "    )\n",
    "\n",
    "def test_paligemma_2(num_images: int = 10):\n",
    "    \"\"\"Test PaliGemma_2 model\"\"\"\n",
    "    return test_single_model(\n",
    "        model_path=\"/content/drive/MyDrive/paligemma_2\",\n",
    "        model_name=\"PaliGemma_2\",\n",
    "        num_images=num_images\n",
    "    )\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🎯 Single Model PaliGemma Inference\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Available functions:\")\n",
    "    print(\"• test_paligemma_1(10)  - Test PaliGemma_1 model\")\n",
    "    print(\"• test_paligemma_2(10)  - Test PaliGemma_2 model\")\n",
    "    print(\"• test_single_model(path, name, num) - Custom model\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Test PaliGemma_1 by default\n",
    "    results = test_paligemma_1(num_images=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paligemma 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "# Core ML libraries\n",
    "from transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration\n",
    "from peft import PeftModel\n",
    "import supervision as sv\n",
    "\n",
    "# Suppress deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('default')\n",
    "\n",
    "@dataclass\n",
    "class SingleModelConfig:\n",
    "    \"\"\"Configuration for single model inference\"\"\"\n",
    "    # Model path - CHANGE THIS TO YOUR MODEL\n",
    "    model_path: str = \"/content/drive/MyDrive/paligemma_2\"  # Changed to paligemma_2\n",
    "    model_name: str = \"PaliGemma_2\"  # Friendly name\n",
    "\n",
    "    # Dataset paths\n",
    "    test_path: str = \"Plastic-Detection-5/dataset/_annotations.test.jsonl\"\n",
    "    dataset_root: str = \"Plastic-Detection-5/dataset\"\n",
    "\n",
    "    # Settings\n",
    "    num_images: int = 10\n",
    "\n",
    "    # Generation settings\n",
    "    max_new_tokens: int = 256\n",
    "    do_sample: bool = False\n",
    "    temperature: float = 0.0\n",
    "    num_beams: int = 1\n",
    "\n",
    "    # Device settings\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    torch_dtype: torch.dtype = torch.bfloat16\n",
    "\n",
    "    # Output settings\n",
    "    output_dir: str = \"single_model_results_p2\"\n",
    "\n",
    "class SingleModelInference:\n",
    "    \"\"\"Simple single model inference\"\"\"\n",
    "\n",
    "    def __init__(self, config: SingleModelConfig):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "        self.test_data = []\n",
    "        self.classes = ['plastic']\n",
    "        self.results = []\n",
    "\n",
    "        # Create output directory\n",
    "        Path(self.config.output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "        print(\"=\"*60)\n",
    "        print(\"🎯 SINGLE MODEL PALIGEMMA INFERENCE\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"📁 Model: {self.config.model_name}\")\n",
    "        print(f\"📂 Path: {self.config.model_path}\")\n",
    "        print(f\"🖼️  Images: {self.config.num_images}\")\n",
    "        print(f\"💾 Device: {self.config.device}\")\n",
    "        if torch.cuda.is_available():\n",
    "            total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            print(f\"🔥 GPU Memory: {total_memory:.1f}GB available\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "    def cleanup_memory(self):\n",
    "        \"\"\"Clean up GPU memory\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "        gc.collect()\n",
    "\n",
    "    def detect_base_model(self) -> str:\n",
    "        \"\"\"Auto-detect base model\"\"\"\n",
    "        try:\n",
    "            config_paths = [\n",
    "                Path(self.config.model_path) / \"training_config.json\",\n",
    "                Path(self.config.model_path) / \"adapter_config.json\",\n",
    "                Path(self.config.model_path) / \"config.json\"\n",
    "            ]\n",
    "\n",
    "            for config_path in config_paths:\n",
    "                if config_path.exists():\n",
    "                    print(f\"   📄 Found config: {config_path.name}\")\n",
    "                    with open(config_path, 'r') as f:\n",
    "                        config = json.load(f)\n",
    "\n",
    "                    # Look for base model\n",
    "                    keys = ['model_id', 'base_model', '_name_or_path', 'model_name_or_path']\n",
    "                    for key in keys:\n",
    "                        if key in config and config[key]:\n",
    "                            print(f\"   🎯 Found base model: {config[key]}\")\n",
    "                            return config[key]\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️  Error reading config: {e}\")\n",
    "\n",
    "        # Fallback based on path\n",
    "        if \"paligemma2\" in self.config.model_path.lower() or \"paligemma_2\" in self.config.model_path:\n",
    "            fallback = \"google/paligemma2-3b-pt-224\"\n",
    "        else:\n",
    "            fallback = \"google/paligemma-3b-pt-224\"\n",
    "\n",
    "        print(f\"   🔄 Using fallback: {fallback}\")\n",
    "        return fallback\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Load the model\"\"\"\n",
    "        print(\"\\n🔄 Loading model...\")\n",
    "\n",
    "        # Check if path exists\n",
    "        if not Path(self.config.model_path).exists():\n",
    "            raise FileNotFoundError(f\"❌ Model path not found: {self.config.model_path}\")\n",
    "\n",
    "        print(f\"   ✅ Model path exists: {self.config.model_path}\")\n",
    "\n",
    "        try:\n",
    "            # Detect base model\n",
    "            base_model_id = self.detect_base_model()\n",
    "\n",
    "            # Load processor\n",
    "            print(\"   📝 Loading processor...\")\n",
    "            try:\n",
    "                self.processor = PaliGemmaProcessor.from_pretrained(self.config.model_path)\n",
    "                print(\"   ✅ Loaded processor from fine-tuned model\")\n",
    "            except:\n",
    "                self.processor = PaliGemmaProcessor.from_pretrained(base_model_id)\n",
    "                print(\"   ✅ Loaded processor from base model\")\n",
    "\n",
    "            # Load base model\n",
    "            print(f\"   🧠 Loading base model: {base_model_id}\")\n",
    "            base_model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "                base_model_id,\n",
    "                torch_dtype=self.config.torch_dtype,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "\n",
    "            # Load LoRA weights\n",
    "            print(\"   🔧 Loading LoRA weights...\")\n",
    "            self.model = PeftModel.from_pretrained(base_model, self.config.model_path)\n",
    "\n",
    "            # Try to merge for faster inference\n",
    "            try:\n",
    "                print(\"   🔗 Merging LoRA weights...\")\n",
    "                self.model = self.model.merge_and_unload()\n",
    "                print(\"   ✅ LoRA weights merged!\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️  Could not merge: {e}\")\n",
    "\n",
    "            self.model.eval()\n",
    "\n",
    "            # Show memory usage\n",
    "            if torch.cuda.is_available():\n",
    "                allocated = torch.cuda.memory_allocated() / 1e9\n",
    "                print(f\"   💾 GPU Memory: {allocated:.2f}GB\")\n",
    "\n",
    "            print(\"   ✅ Model loaded successfully!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error loading model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def load_test_data(self):\n",
    "        \"\"\"Load test dataset\"\"\"\n",
    "        print(\"\\n📂 Loading test dataset...\")\n",
    "\n",
    "        if not Path(self.config.test_path).exists():\n",
    "            raise FileNotFoundError(f\"❌ Test file not found: {self.config.test_path}\")\n",
    "\n",
    "        # Load data\n",
    "        with open(self.config.test_path, 'r', encoding='utf-8') as f:\n",
    "            all_data = [json.loads(line) for line in f]\n",
    "\n",
    "        # Select images\n",
    "        self.test_data = all_data[:self.config.num_images]\n",
    "        print(f\"   📊 Selected {len(self.test_data)} images\")\n",
    "\n",
    "        # Pre-load images\n",
    "        print(\"   🖼️  Loading images...\")\n",
    "        self.images = []\n",
    "        self.image_info = []\n",
    "\n",
    "        for item in tqdm(self.test_data, desc=\"Loading images\"):\n",
    "            img_path = Path(self.config.dataset_root) / item['image']\n",
    "            if img_path.exists():\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "                # Resize if needed\n",
    "                max_size = 1024\n",
    "                if max(image.size) > max_size:\n",
    "                    ratio = max_size / max(image.size)\n",
    "                    new_size = (int(image.width * ratio), int(image.height * ratio))\n",
    "                    image = image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "                self.images.append(image)\n",
    "                self.image_info.append({\n",
    "                    'path': item['image'],\n",
    "                    'prompt': \"<image>\" + item['prefix'],\n",
    "                    'gt_text': item['suffix'],\n",
    "                    'size': image.size\n",
    "                })\n",
    "\n",
    "        print(f\"   ✅ Loaded {len(self.images)} images\")\n",
    "\n",
    "    def generate_prediction(self, image: Image.Image, prompt: str) -> str:\n",
    "        \"\"\"Generate prediction for image\"\"\"\n",
    "        try:\n",
    "            # Prepare inputs\n",
    "            inputs = self.processor(\n",
    "                text=prompt,\n",
    "                images=image,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=256\n",
    "            ).to(self.config.device)\n",
    "\n",
    "            # Convert to correct dtype\n",
    "            if 'pixel_values' in inputs:\n",
    "                inputs['pixel_values'] = inputs['pixel_values'].to(self.config.torch_dtype)\n",
    "\n",
    "            prefix_length = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "            # Generate\n",
    "            with torch.inference_mode():\n",
    "                generation = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=self.config.max_new_tokens,\n",
    "                    do_sample=self.config.do_sample,\n",
    "                    num_beams=self.config.num_beams,\n",
    "                    temperature=self.config.temperature,\n",
    "                    pad_token_id=self.processor.tokenizer.eos_token_id,\n",
    "                    eos_token_id=self.processor.tokenizer.eos_token_id,\n",
    "                    use_cache=True,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "\n",
    "            # Decode\n",
    "            new_tokens = generation[0][prefix_length:]\n",
    "            generated_text = self.processor.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "            return generated_text.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Generation error: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    def parse_detections(self, result_text: str, image_size: Tuple[int, int]) -> sv.Detections:\n",
    "        \"\"\"Parse detections using updated supervision API\"\"\"\n",
    "        try:\n",
    "            if \"Error\" in result_text:\n",
    "                return sv.Detections.empty()\n",
    "\n",
    "            # Updated to use from_vlm instead of from_lmm\n",
    "            detections = sv.Detections.from_vlm(\n",
    "                vlm='paligemma',  # Changed from 'lmm' to 'vlm'\n",
    "                result=result_text,\n",
    "                resolution_wh=image_size,\n",
    "                classes=self.classes\n",
    "            )\n",
    "\n",
    "            if len(detections) > 0:\n",
    "                detections.confidence = np.ones(len(detections))\n",
    "                if not hasattr(detections, 'class_id') or detections.class_id is None:\n",
    "                    detections.class_id = np.zeros(len(detections), dtype=int)\n",
    "\n",
    "            return detections\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️  Parse error for '{result_text[:50]}...': {e}\")\n",
    "            return sv.Detections.empty()\n",
    "\n",
    "    def run_inference(self):\n",
    "        \"\"\"Run inference on all images\"\"\"\n",
    "        print(\"\\n🎯 Running inference...\")\n",
    "\n",
    "        self.results = []\n",
    "\n",
    "        for i, (image, info) in enumerate(tqdm(zip(self.images, self.image_info),\n",
    "                                              desc=\"Processing images\",\n",
    "                                              total=len(self.images))):\n",
    "            try:\n",
    "                # Generate prediction\n",
    "                pred_text = self.generate_prediction(image, info['prompt'])\n",
    "\n",
    "                # Parse detections\n",
    "                detections = self.parse_detections(pred_text, info['size'])\n",
    "                gt_detections = self.parse_detections(info['gt_text'], info['size'])\n",
    "\n",
    "                # Store result\n",
    "                result = {\n",
    "                    'image_path': info['path'],\n",
    "                    'image': image,\n",
    "                    'prompt': info['prompt'],\n",
    "                    'generated_text': pred_text,\n",
    "                    'ground_truth_text': info['gt_text'],\n",
    "                    'predicted_detections': detections,\n",
    "                    'ground_truth_detections': gt_detections,\n",
    "                    'predicted_count': len(detections),\n",
    "                    'ground_truth_count': len(gt_detections)\n",
    "                }\n",
    "\n",
    "                self.results.append(result)\n",
    "\n",
    "                # Print progress\n",
    "                if (i + 1) % 5 == 0 or i == 0:\n",
    "                    print(f\"   📊 Image {i+1}: Generated '{pred_text[:50]}...', Detections: {len(detections)}\")\n",
    "\n",
    "                # Clear cache\n",
    "                if (i + 1) % 3 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Error processing image {i}: {e}\")\n",
    "                # Add empty result\n",
    "                self.results.append({\n",
    "                    'image_path': info['path'],\n",
    "                    'image': image,\n",
    "                    'generated_text': f\"Error: {str(e)}\",\n",
    "                    'predicted_detections': sv.Detections.empty(),\n",
    "                    'ground_truth_detections': sv.Detections.empty(),\n",
    "                    'predicted_count': 0,\n",
    "                    'ground_truth_count': 0\n",
    "                })\n",
    "\n",
    "        total_predictions = sum(r['predicted_count'] for r in self.results)\n",
    "        total_ground_truth = sum(r['ground_truth_count'] for r in self.results)\n",
    "\n",
    "        print(f\"\\n   📊 Inference completed!\")\n",
    "        print(f\"   🎯 Total predictions: {total_predictions}\")\n",
    "        print(f\"   📋 Total ground truth: {total_ground_truth}\")\n",
    "        print(f\"   📈 Average per image: {total_predictions/len(self.results):.2f}\")\n",
    "\n",
    "    def create_visualization(self):\n",
    "        \"\"\"Create visualization of results\"\"\"\n",
    "        print(\"\\n🎨 Creating visualization...\")\n",
    "\n",
    "        # Create grid\n",
    "        cols = 2  # Ground Truth | Prediction\n",
    "        rows = len(self.results)\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(12, 4*rows))\n",
    "\n",
    "        if rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "\n",
    "        for i, result in enumerate(self.results):\n",
    "            # Ground Truth\n",
    "            ax_gt = axes[i, 0] if rows > 1 else axes[0]\n",
    "            ax_gt.imshow(result['image'])\n",
    "\n",
    "            # Draw GT boxes in green\n",
    "            for bbox in result['ground_truth_detections'].xyxy:\n",
    "                x1, y1, x2, y2 = bbox\n",
    "                rect = plt.Rectangle((x1, y1), x2-x1, y2-y1,\n",
    "                                   linewidth=2, edgecolor='green',\n",
    "                                   facecolor='none', alpha=0.8)\n",
    "                ax_gt.add_patch(rect)\n",
    "\n",
    "            ax_gt.set_title(f'Ground Truth\\nDetections: {result[\"ground_truth_count\"]}')\n",
    "            ax_gt.axis('off')\n",
    "\n",
    "            # Predictions\n",
    "            ax_pred = axes[i, 1] if rows > 1 else axes[1]\n",
    "            ax_pred.imshow(result['image'])\n",
    "\n",
    "            # Draw prediction boxes in red\n",
    "            for bbox in result['predicted_detections'].xyxy:\n",
    "                x1, y1, x2, y2 = bbox\n",
    "                rect = plt.Rectangle((x1, y1), x2-x1, y2-y1,\n",
    "                                   linewidth=2, edgecolor='red',\n",
    "                                   facecolor='none', alpha=0.8)\n",
    "                ax_pred.add_patch(rect)\n",
    "\n",
    "            ax_pred.set_title(f'Prediction\\nDetections: {result[\"predicted_count\"]}')\n",
    "            ax_pred.axis('off')\n",
    "\n",
    "        plt.suptitle(f'{self.config.model_name} Results', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save\n",
    "        vis_path = Path(self.config.output_dir) / f\"{self.config.model_name}_results.png\"\n",
    "        plt.savefig(vis_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"   💾 Saved: {vis_path}\")\n",
    "        plt.show()\n",
    "\n",
    "    def print_summary(self):\n",
    "        \"\"\"Print detailed summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"📊 INFERENCE SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        print(f\"\\n🎯 Model: {self.config.model_name}\")\n",
    "        print(f\"📂 Path: {self.config.model_path}\")\n",
    "        print(f\"🖼️  Images processed: {len(self.results)}\")\n",
    "\n",
    "        # Statistics\n",
    "        total_pred = sum(r['predicted_count'] for r in self.results)\n",
    "        total_gt = sum(r['ground_truth_count'] for r in self.results)\n",
    "\n",
    "        print(f\"\\n📈 Detection Statistics:\")\n",
    "        print(f\"   Predicted: {total_pred} ({total_pred/len(self.results):.2f} avg)\")\n",
    "        print(f\"   Ground Truth: {total_gt} ({total_gt/len(self.results):.2f} avg)\")\n",
    "\n",
    "        # Per-image breakdown\n",
    "        print(f\"\\n📋 Per-Image Results:\")\n",
    "        print(\"Image\".ljust(30) + \"GT\".ljust(6) + \"Pred\".ljust(6) + \"Generated Text\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        for result in self.results:\n",
    "            img_name = Path(result['image_path']).name[:25]\n",
    "            gt_count = result['ground_truth_count']\n",
    "            pred_count = result['predicted_count']\n",
    "            gen_text = result['generated_text'][:30]\n",
    "\n",
    "            print(f\"{img_name}...\".ljust(30) +\n",
    "                  f\"{gt_count}\".ljust(6) +\n",
    "                  f\"{pred_count}\".ljust(6) +\n",
    "                  f\"{gen_text}...\")\n",
    "\n",
    "        print(\"=\"*60)\n",
    "\n",
    "    def save_results(self):\n",
    "        \"\"\"Save detailed results\"\"\"\n",
    "        print(\"\\n💾 Saving results...\")\n",
    "\n",
    "        # Prepare data for JSON\n",
    "        json_results = []\n",
    "        for result in self.results:\n",
    "            json_result = {\n",
    "                'image_path': result['image_path'],\n",
    "                'prompt': result['prompt'],\n",
    "                'generated_text': result['generated_text'],\n",
    "                'ground_truth_text': result['ground_truth_text'],\n",
    "                'predicted_count': result['predicted_count'],\n",
    "                'ground_truth_count': result['ground_truth_count'],\n",
    "                'predicted_boxes': result['predicted_detections'].xyxy.tolist() if len(result['predicted_detections']) > 0 else [],\n",
    "                'ground_truth_boxes': result['ground_truth_detections'].xyxy.tolist() if len(result['ground_truth_detections']) > 0 else []\n",
    "            }\n",
    "            json_results.append(json_result)\n",
    "\n",
    "        # Save\n",
    "        results_path = Path(self.config.output_dir) / f\"{self.config.model_name}_detailed_results.json\"\n",
    "        with open(results_path, 'w') as f:\n",
    "            json.dump(json_results, f, indent=2)\n",
    "\n",
    "        print(f\"   💾 Saved: {results_path}\")\n",
    "\n",
    "    def run_complete_inference(self):\n",
    "        \"\"\"Run complete inference pipeline\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            # Load model and data\n",
    "            self.load_model()\n",
    "            self.load_test_data()\n",
    "\n",
    "            # Run inference\n",
    "            self.run_inference()\n",
    "\n",
    "            # Create outputs\n",
    "            self.create_visualization()\n",
    "            self.print_summary()\n",
    "            self.save_results()\n",
    "\n",
    "            total_time = time.time() - start_time\n",
    "            print(f\"\\n🎉 INFERENCE COMPLETED!\")\n",
    "            print(f\"⏱️  Total time: {total_time:.1f} seconds\")\n",
    "            print(f\"📁 Results saved in: {self.config.output_dir}/\")\n",
    "\n",
    "            return self.results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n💥 Inference failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "# Main functions\n",
    "def test_single_model(model_path: str,\n",
    "                     model_name: str = None,\n",
    "                     num_images: int = 10):\n",
    "    \"\"\"\n",
    "    Test a single model\n",
    "\n",
    "    Args:\n",
    "        model_path: Path to your model (e.g., \"/content/drive/MyDrive/paligemma_1\")\n",
    "        model_name: Friendly name (optional)\n",
    "        num_images: Number of images to test\n",
    "    \"\"\"\n",
    "\n",
    "    if model_name is None:\n",
    "        model_name = Path(model_path).name\n",
    "\n",
    "    config = SingleModelConfig(\n",
    "        model_path=model_path,\n",
    "        model_name=model_name,\n",
    "        num_images=num_images\n",
    "    )\n",
    "\n",
    "    inferencer = SingleModelInference(config)\n",
    "    return inferencer.run_complete_inference()\n",
    "\n",
    "# Quick test functions\n",
    "def test_paligemma_1(num_images: int = 10):\n",
    "    \"\"\"Test PaliGemma_1 model\"\"\"\n",
    "    return test_single_model(\n",
    "        model_path=\"/content/drive/MyDrive/paligemma_1\",\n",
    "        model_name=\"PaliGemma_1\",\n",
    "        num_images=num_images\n",
    "    )\n",
    "\n",
    "def test_paligemma_2(num_images: int = 10):\n",
    "    \"\"\"Test PaliGemma_2 model\"\"\"\n",
    "    return test_single_model(\n",
    "        model_path=\"/content/drive/MyDrive/paligemma_2\",\n",
    "        model_name=\"PaliGemma_2\",\n",
    "        num_images=num_images\n",
    "    )\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🎯 Single Model PaliGemma Inference\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Available functions:\")\n",
    "    print(\"• test_paligemma_1(10)  - Test PaliGemma_1 model\")\n",
    "    print(\"• test_paligemma_2(10)  - Test PaliGemma_2 model\")\n",
    "    print(\"• test_single_model(path, name, num) - Custom model\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Test PaliGemma_2\n",
    "    results = test_paligemma_2(num_images=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
